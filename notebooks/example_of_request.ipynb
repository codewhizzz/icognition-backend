{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "path_file = '/home/eboraks/Projects/icognition_backend/data/icog_pages/bergum.medium.com%2Ffour-mistakes-when-introducing-embeddings-and-vector-search-d39478a568c5.json'\n",
    "with open(path_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://bergum.medium.com/four-mistakes-when-introducing-embeddings-and-vector-search-d39478a568c5#',\n",
       " 'title': None,\n",
       " 'clean_url': 'bergum.medium.com%2Ffour-mistakes-when-introducing-embeddings-and-vector-search-d39478a568c5',\n",
       " 'paragraphs': {'8': 'Representing unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?',\n",
       "  '9': 'Embeddings are learned transformations to make data more useful',\n",
       "  '10': 'In academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).',\n",
       "  '11': 'How useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.',\n",
       "  '12': 'So what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.',\n",
       "  '13': 'About self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.',\n",
       "  '14': 'This type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.',\n",
       "  '15': 'To shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?',\n",
       "  '16': 'BERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.',\n",
       "  '17': 'We can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.',\n",
       "  '18': 'The BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.',\n",
       "  '19': 'Now, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.',\n",
       "  '20': 'Now we have an embedding representation of a text chunk, which leads to mistake number 1.',\n",
       "  '21': 'Using the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.',\n",
       "  '22': 'Encoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.',\n",
       "  '23': 'To obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.',\n",
       "  '24': 'This fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.',\n",
       "  '25': 'The problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.',\n",
       "  '26': 'The BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.',\n",
       "  '27': 'We studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.',\n",
       "  '28': 'I’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.',\n",
       "  '29': 'So you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.',\n",
       "  '30': 'The first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.',\n",
       "  '31': 'On the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.',\n",
       "  '32': 'Given the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?',\n",
       "  '33': 'Let us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.',\n",
       "  '34': 'On the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.',\n",
       "  '35': 'An exhaustive search might be all you need',\n",
       "  '36': \"The exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\",\n",
       "  '37': 'For example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.',\n",
       "  '39': 'Going down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.',\n",
       "  '40': 'The above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.',\n",
       "  '41': 'If we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.',\n",
       "  '42': 'What is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.',\n",
       "  '43': 'If you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.'},\n",
       " 'summarized_paragraphs': None,\n",
       " 'key_concepts': None,\n",
       " 'full_text': \"Representing unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\nRepresenting unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. What are embeddings anyway? Roy Keyes explains it well in The shortest definition of embeddings?\\nEmbeddings are learned transformations to make data more useful\\nIn academia, this process is known as representation learning and has been a field of research for decades. By transforming the data into vectors, a language native to computers, we can make the data more useful. Take BERT for text as an example. Bidirectional Encoder Representations from Transformers (BERT).\\nHow useful the representation is, depends on how we learn this transformation and how the learned way to represent data generalizes to new data. This is how we do Machine Learning. Take some data, learn something from it, then apply that learning to new data. Simple.\\nSo what is new? Why the surge in interest? The answer is better model architectures (e.g., Transformer architecture) and self-supervised representation learning. Add a touch of confusion around Large Language Models (LLMs) such as chatGPT to the mix, and here we are.\\nAbout self-supervised learning. Using a clever objective, we can train a model using piles of data without human supervision (labeling). Then, once that is done, we can fine-tune the model for tasks where the fine-tuning requires less labeled data than if we started from scratch.\\nThis type of learning pipelining is called transfer learning. Learning to snowboard also transfers to skateboarding, windsurfing, surfing, and other fun activities.\\nTo shorten this blog post, let us focus on text models and BERT models specifically. How can we transform data into useful embedding representation using Transformer-based models?\\nBERT is a deep neural network model with weights, layers, and whatnot, a complexity we hide inside the box. If we pull down the model from Huggingface, the model weights are assigned by pre-training using a masked language model objective.\\nWe can take some text and tokenize that text into a fixed vocabulary to obtain a set of numeric ids. A mapping between free text and hard-coded identifiers. The vocabulary size depends on the language, but for the vanilla BERT model for English, this is around 30K words. Unknown words (out of vocabulary) are assigned UNK and given a specially reserved identifier. All unknown words are assigned to this identifier, and the model cannot differentiate between “foo” and “bar” if both are not in the vocabulary.\\nThe BERT model can take a maximum of 512 words (input context length limitation), and the network output is 512 vectors with dimensionality N, depending on the type of bert-base model. A vanilla BERT model uses 768 dimensions. For an input of 512 words, we obtain a matrix of 512 x 768 floats, one 768-dimensional vector per input word. Unlike previous NLP model architectures, like Word2vec, each word vector representation on the output is contextualized by the attention mechanism in the Transformer architecture. The vector representation of a single word depends on all the other words in the input.\\nNow, we have multiple vectors representing a single text; what do we do if we want to represent a chunk of text, a text passage, or a paragraph of text in a single vector representation? One approach is to choose a single output vector as the representation and ignore the rest. Another approach is pooling. For example, average pooling will average the 512 output vectors into a single vector representation.\\nNow we have an embedding representation of a text chunk, which leads to mistake number 1.\\nUsing the direct vector representations from the model that have only been pre-trained will not produce a useful embedding representation for any task. Search ranking is an example of such a task; see details in How not to use BERT for search ranking.\\nEncoding free text queries and documents and expecting that the cosine similarity between the two representations can rank the documents by relevance is naive, and the results of that approach give you next to random ranking results. Your learned snowboard skills do not transfer to playing golf or swimming.\\nTo obtain a useful embedding representation (better than random) for search ranking, we need to tune the model weights. We can do that by using a different objective when training the model. We can train (update the weights) using labeled examples like relevant and irrelevant documents for a large sample of queries. MS MARCO is a large web search relevance collection with labeled queries and document pairs, which can be used to train a ranking model.\\nThis fine-tuning creates useful embedding representations based on BERT and outcompetes traditional keyword search methods with no learnable parameters, such as BM25, by a very large margin on the MS MARCO dataset.\\nThe problem is that when we take a single vector representation model, fine-tuned on MS MARCO labels, it does not beat BM25 in a different domain with slightly different types of documents and questions.\\nThe BEIR Benchmark is an excellent framework for evaluating the quality of models trained on MS Marco and how well they transfer to different domains and tasks.\\nWe studied the effectiveness of ten different retrieval models and demonstrate that in-domain performance cannot predict how well an approach will generalize in a zero-shot setup. Many approaches 9 that outperform BM25 in an in-domain evaluation on MS MARCO, perform poorly on the BEIR datasets.\\nI’ve written about zero-shot ranking and some solutions here, here, and here. Multi-vector representation model for search, like ColBERT, generalizes much better than single-vector representations.\\nSo you made it here and have useful embedding representations of data. Now, you need a way to search the vector data using the nearest neighbor search, also known as KNN, and you can deploy your exciting use case to production.\\nThe first thing you should ask yourself is, will we need to introduce an approximate nearest neighbor search (ANNS) instead of an exact nearest neighbor search? As in many aspects of life, this is a question of tradeoffs.\\nOn the query serving side. Even not considering the document side processing complexity, like the need for CRUD, real-time versus batch, etc.\\nGiven the above, it comes down to production deployment cost; how many servers do we need, or do we need servers at all?\\nLet us expand on the accuracy error tolerance and why that is use-case dependent. If you are building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many equally great cat photos, and bringing back the exact best cats as deemed most relevant by the model might not be that important.\\nOn the other hand, if you are building a retina image scan app using vector search to determine if the user can access the building, you better have great overlap@1. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-recall settings.\\nAn exhaustive search might be all you need\\nThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The search can also efficiently be limited to a subset if we store the vectors in an engine with query engine filtering capabilities.\\nFor example, brute force searching 1M vectors with 128 dimensions takes about 100ms single-threaded. We can parallelize the search; for example, by using four threads, we can get it down to 25 ms until memory bandwidth hits. If we page the vector data randomly from the disk, it will be slower but still parallelizable. If we have 10B vectors, and we don’t have a way to efficiently select a subset of documents that we perform the nearest neighbor search over, we have a cost problem. We can still get decent latency by distributing the search over multiple nodes in parallel, as Vespa can do. But renting servers to keep the latency in check can become costly with billions of embeddings. Add high query throughput to the mix, and we have a real cost problem.\\nGoing down the approximate vector search route, we need an algorithm that can index the vector data so that searches are less costly than exhaustive searches at the cost of resource usage and indexing processing. Here there are also many tradeoffs, like disk usage and memory usage. How well the algorithm can be used with real-time CRUD operations. One source of ANN algorithm understanding is https://github.com/erikbern/ann-benchmarks, where different algorithms and implementations are compared on various vector datasets.\\nThe above graph is for the SIFT dataset, with 1M 128-dimensional vectors. The graph displays recall@10 (same as overlap@10) versus the queries per second. The benchmark is single-threaded, which means that if the algorithm is at 10² QPS, we have a latency of 10ms. 10³ QPS means a latency of 1ms, and so forth. These algorithms are pretty damn fast.\\nIf we deploy these algorithms on a server with multiple CPU cores, we can enjoy even more QPS. 2 cores are expected to give 2x QPS, as long as there aren’t any contention or locking scaling problems. But not all ANN algorithms give us equally good recall. Algorithms that are up and to the right give the best tradeoff between performance and accuracy, and the lower left quadrant is worse. As seen above, some algorithms struggle with getting past 50% recall.\\nWhat is not reflected in the graph above is the cost of indexing and whether the algorithm can support updates and CRUD operations. Some are batch-oriented, so they first need a large sample of the document vectors before they can build the index, while others can build the index incrementally. Note that ann-benchmark can only use open-source algorithms to reproduce on the same runtime. Some commercial and proprietary vector search vendors have unknown recall versus performance tradeoffs.\\nIf you hated this post, you could shout out to me over at Twitter https://twitter.com/jobergum.\\n\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = requests.post(\"http://localhost:8889/bookmark\", json=data)\n",
    "print(response.json()['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'A New York woman who has been married 10 times has been charged with marriage fraud.'}]\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\"http://localhost:8888/one_sentance\", json=payload)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but you input_length is only 803. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=401)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representing unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. The answer is better model architectures, transformer architecture, and self-supervised representation learning. Machine Learning uses machine learning to learn from big data in order to apply machine learning techniques to new data. In this post, we'll focus on text models and BERT models specifically. These are deep neural network models that can be used to rank documents without human supervision. For example, snowboarders do well when they're ranked according to relevance; for golfers, however, they do poorly when it's ranked by relevance. We also briefly delve into the use case for KNN, a distributed clustering framework designed to track how many different types of information each dataset contains.\n",
      "Let us expand on the accuracy error tolerance and why that is use-case dependent. If you're building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many great cat photos out there, but bringing back the exact best cats as deemed most relevant by the model might not be that important. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-reall settings. An exhaustive search might be all you needThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The cost of resource usage and indexing processing may be one of the biggest tradeoffs. However, if we store the vectors in an engine with query engine filtering capabilities, we have a good chance of getting the same result. Using vector datasets, Erik Bjornson compares different ANN algorithms and implementations to compare them. He notes that some commercial and proprietary vector search vendors have unknown \"quality versus performance tradeoffs\" .\n"
     ]
    }
   ],
   "source": [
    "sentences = list(data['paragraphs'].values())\n",
    "chunks = concatenate_sentences_to_chunks(sentences)\n",
    "\n",
    "for chunk in chunks:\n",
    "    summary = pszemraj_summarizer(chunk)\n",
    "    print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_sentences_to_chunks(sentences):\n",
    "  \n",
    "  chunks = []\n",
    "  current_chunk = ''\n",
    "  for sentence in sentences:\n",
    "    current_chunk += sentence.replace('\\n', ' ')\n",
    "    if len(current_chunk.split(' ')) >= 1000:\n",
    "      chunks.append(current_chunk)\n",
    "      current_chunk = ''\n",
    "\n",
    "  chunks.append(current_chunk)\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keyphrase extraction pipeline\n",
    "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )     \n",
    "\n",
    "\n",
    "    def postprocess(self, all_outputs):\n",
    "        mid_results = super().postprocess(\n",
    "            all_outputs=all_outputs,\n",
    "            aggregation_strategy = AggregationStrategy.AVERAGE,\n",
    "        )\n",
    "        strings = set()\n",
    "        results = []\n",
    "        for kp in mid_results:\n",
    "            if kp.get(\"word\") not in strings:\n",
    "                strings.add(kp.get(\"word\"))\n",
    "                results.append(kp)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(kp_objects, st_model):\n",
    "    keyphrases = [kp.get(\"word\") for kp in kp_objects]\n",
    "    embeddings = st_model.encode(keyphrases)\n",
    "\n",
    "    #Compute cosine similarity between all pairs\n",
    "    cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "    #Add all pairs to a list with their cosine similarity score\n",
    "    all_sentence_combinations = []\n",
    "    for i in range(len(cos_sim)-1):\n",
    "        for j in range(i+1, len(cos_sim)):\n",
    "            all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "    #Sort list by the highest cosine similarity score\n",
    "    all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    print(\"Top-5 most similar pairs:\")\n",
    "    for score, i, j in all_sentence_combinations:\n",
    "        print(\"{} \\t {} \\t {:.4f}\".format(keyphrases[i], keyphrases[j], cos_sim[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ml6team/keyphrase-extraction-distilbert-inspec\"\n",
    "extractor = KeyphraseExtractionPipeline(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'KEY', 'score': 0.82937986, 'word': 'unstructured data', 'start': 13, 'end': 30}\n",
      "{'entity_group': 'KEY', 'score': 0.8330004, 'word': 'embedding vectors', 'start': 34, 'end': 51}\n",
      "{'entity_group': 'KEY', 'score': 0.995934, 'word': 'vector search', 'start': 94, 'end': 107}\n",
      "{'entity_group': 'KEY', 'score': 0.74980867, 'word': 'embeddings', 'start': 220, 'end': 230}\n",
      "{'entity_group': 'KEY', 'score': 0.38632116, 'word': 'ebr', 'start': 315, 'end': 318}\n",
      "{'entity_group': 'KEY', 'score': 0.8314931, 'word': 'roy keyes', 'start': 395, 'end': 404}\n",
      "{'entity_group': 'KEY', 'score': 0.5396693, 'word': 'embedding', 'start': 562, 'end': 571}\n",
      "{'entity_group': 'KEY', 'score': 0.42835164, 'word': 'vectors', 'start': 572, 'end': 579}\n",
      "{'entity_group': 'KEY', 'score': 0.917883, 'word': 'transformations', 'start': 783, 'end': 798}\n",
      "{'entity_group': 'KEY', 'score': 0.99932146, 'word': 'academia', 'start': 827, 'end': 835}\n",
      "{'entity_group': 'KEY', 'score': 0.9998516, 'word': 'representation learning', 'start': 862, 'end': 885}\n",
      "{'entity_group': 'KEY', 'score': 0.8128293, 'word': 'bidirectional encoder representations', 'start': 1071, 'end': 1108}\n",
      "{'entity_group': 'KEY', 'score': 0.54438907, 'word': ')', 'start': 1132, 'end': 1133}\n",
      "{'entity_group': 'KEY', 'score': 0.806793, 'word': 'language', 'start': 1580, 'end': 1588}\n",
      "{'entity_group': 'KEY', 'score': 0.99991655, 'word': 'machine learning', 'start': 1905, 'end': 1921}\n",
      "{'entity_group': 'KEY', 'score': 0.99486095, 'word': 'search', 'start': 2111, 'end': 2117}\n",
      "{'entity_group': 'KEY', 'score': 0.5016423, 'word': 'keyes', 'start': 2177, 'end': 2182}\n",
      "{'entity_group': 'KEY', 'score': 0.9250193, 'word': 'learning', 'start': 2359, 'end': 2367}\n"
     ]
    }
   ],
   "source": [
    "kps = extractor(data['full_text'])\n",
    "for kp in kps:\n",
    "    print(kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "embeddings \t embedding \t 0.9631\n",
      "embedding vectors \t embedding \t 0.8369\n",
      "embedding vectors \t embeddings \t 0.8364\n",
      "vector search \t vectors \t 0.6999\n",
      "roy keyes \t keyes \t 0.6559\n"
     ]
    }
   ],
   "source": [
    "prune(kps, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.777390566811793"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = nlp('machine learning')\n",
    "l = nlp('representation learning')\n",
    "ml.similarity(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity: tensor([[0.6312]])\n"
     ]
    }
   ],
   "source": [
    "#Sentences are encoded by calling model.encode()\n",
    "emb1 = model.encode('machine learning')\n",
    "emb2 = model.encode('learning')\n",
    "\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://medium.com/@michaelgoitein/the-playing-to-win-framework-part-iii-the-strategy-choice-cascade-279c3b5ccea3\"\n",
    "res = requests.get(url)\n",
    "content = res.text\n",
    "soup = BeautifulSoup(content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artilces = soup.find('article')\n",
    "\n",
    "main_article = None\n",
    "content_estimator = []\n",
    "for article in artilces:\n",
    "    num_contect_ele = (len(article.find_all('h1')) + len(article.find_all('p')))\n",
    "    content_estimator.append(num_contect_ele)\n",
    "\n",
    "print(max(content_estimator))\n",
    "content_estimator.index(max(content_estimator)) \n",
    "                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"l\"><div class=\"l\"><span class=\"l\"></span><section><div><div class=\"eh ei ej ek el em\"></div><div><div class=\"speechify-ignore l\"><div class=\"eo ep eq er es l\"></div><div class=\"ab ca\"><div class=\"ch bg dx dy dz ea\"><div class=\"ck l\"><div class=\"ev ab\"><div aria-hidden=\"false\" class=\"bl\"><button aria-label=\"Member-only story\" class=\"l ax ao am\"><div class=\"h k j i d\"><div><div aria-hidden=\"false\" class=\"bl\"><svg fill=\"none\" height=\"16\" viewbox=\"0 0 20 20\" width=\"16\"><path d=\"M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z\" fill=\"#FFC017\"></path></svg></div></div></div><div class=\"s u w ew ex q\"><svg class=\"et eu\" fill=\"none\" height=\"16\" viewbox=\"0 0 20 20\" width=\"16\"><path d=\"M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z\" fill=\"#FFC017\"></path></svg><p class=\"be b bf z ey\">Member-only story</p></div></button></div></div></div></div></div></div></div><div class=\"ez fa fb fc fd\"><div class=\"ab ca\"><div class=\"ch bg dx dy dz ea\"><div><h1 class=\"pw-post-title fe ff fg be fh fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx fy bj\" data-testid=\"storyTitle\" id=\"719a\">The “Playing to Win” Framework — Part III — The Strategy Choice Cascade</h1></div><div><h2 class=\"pw-subtitle-paragraph fz ff fg be b ga gb gc gd ge gf gg gh gi gj gk gl gm gn go cp ey\" id=\"2993\">A global strategy expert’s simple but effective approach to designing winning strategies</h2><div class=\"gp gq gr gs gt\"><div class=\"speechify-ignore ab co\"><div class=\"speechify-ignore bg l\"><div class=\"gu gv gw gx gy ab\"><div><div class=\"ab gz\"><a href=\"/@michaelgoitein?source=post_page-----279c3b5ccea3--------------------------------\" rel=\"noopener follow\"><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"l ha hb bx hc hd\"><div class=\"l hh\"><img alt=\"Michael Goitein\" class=\"l ec bx dc dd cw\" data-testid=\"authorPhoto\" height=\"44\" loading=\"lazy\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*YjVBsYD-ACBzetS28w0xZg.jpeg\" width=\"44\"/><div class=\"he bx l dc dd eh n hf hg\"></div></div></div></div></div></a></div></div><div class=\"bm bg l\"><div class=\"ab\"><div style=\"flex:1\"><span class=\"be b bf z bj\"><div class=\"hi ab q\"><div class=\"ab q hj\"><div class=\"ab q\"><div><div aria-hidden=\"false\" class=\"bl\"><p class=\"be b hk hl bj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar hm\" data-testid=\"authorName\" href=\"/@michaelgoitein?source=post_page-----279c3b5ccea3--------------------------------\" rel=\"noopener follow\">Michael Goitein</a></p></div></div></div><span aria-hidden=\"true\" class=\"hn ho\"><span class=\"be b bf z ey\">·</span></span><p class=\"be b hk hl ey\"><span><a class=\"hp hq ah ai aj ak al am an ao ap aq ar hr hs ht\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68c04f19aae2&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40michaelgoitein%2Fthe-playing-to-win-framework-part-iii-the-strategy-choice-cascade-279c3b5ccea3&amp;user=Michael+Goitein&amp;userId=68c04f19aae2&amp;source=post_page-68c04f19aae2----279c3b5ccea3---------------------post_header-----------\" rel=\"noopener follow\">Follow</a></span></p></div></div></span></div></div><div class=\"l hu\"><span class=\"be b bf z ey\"><div class=\"ab cm hv hw hx\"><span class=\"be b bf z ey\"><div class=\"ab ae\"><span data-testid=\"storyReadTime\">13 min read</span><div aria-hidden=\"true\" class=\"hy hz l\"><span aria-hidden=\"true\" class=\"l\"><span class=\"be b bf z ey\">·</span></span></div><span data-testid=\"storyPublishDate\">Feb 24</span></div></span></div></span></div></div></div><div class=\"ab co ia ib ic id ie if ig ih ii ij ik il im in io ip\"><div class=\"h k w ew ex q\"><div class=\"jf l\"><div class=\"ab q jg\"><div class=\"pw-multi-vote-icon hh et jh ji jj\"><div class=\"\"><div class=\"jk jl jm jn jo jp jq am jr js jt jj\"><svg aria-label=\"clap\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z\" fill-rule=\"evenodd\"></path><path clip-rule=\"evenodd\" d=\"M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z\" fill-rule=\"evenodd\"></path></svg></div></div></div><div class=\"pw-multi-vote-count l ju jv jw jx jy jz ka\"><p class=\"be b kb z ey\"><span class=\"jl\">--</span></p></div></div></div><div><div aria-hidden=\"false\" class=\"bl\"><button aria-label=\"responses\" class=\"ao jk kc kd ab q ke kf kg\" data-testid=\"headerResponseButton\"><svg class=\"fx\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg></button></div></div></div><div class=\"ab q iq ir is it iu iv iw ix iy iz ja jb jc jd je\"><div class=\"kh k j i d\"></div><div class=\"h k\"></div><div class=\"ec qs cm\"><div class=\"l ae\"><div class=\"ab ca\"><div class=\"qt qu qv qw qx ln ch bg\"><div class=\"ab\"></div></div></div></div></div><div aria-describedby=\"postFooterSocialMenu\" aria-hidden=\"false\" aria-labelledby=\"postFooterSocialMenu\" class=\"bl\"><div><div aria-hidden=\"false\" class=\"bl\"><button aria-controls=\"postFooterSocialMenu\" aria-expanded=\"false\" aria-label=\"Share Post\" class=\"af ke ah ai aj ak al ki an ao ap hr kj kk kg kl km kn ko kp s kq kr ks kt ku kv kw u kx ky kz\" data-testid=\"headerSocialShareButton\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path></svg><div class=\"j i d\"><p class=\"be b bf z ey\">Share</p></div></button></div></div></div></div></div></div></div></div></div><figure class=\"ld le lf lg lh li la lb paragraph-image\"><div class=\"lj lk hh ll bg lm\" role=\"button\" tabindex=\"0\"><div class=\"la lb lc\"><picture><source sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*r5TuRNbUKytQm--do3BgUA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*r5TuRNbUKytQm--do3BgUA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*r5TuRNbUKytQm--do3BgUA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*r5TuRNbUKytQm--do3BgUA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*r5TuRNbUKytQm--do3BgUA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*r5TuRNbUKytQm--do3BgUA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r5TuRNbUKytQm--do3BgUA.jpeg 1400w\" type=\"image/webp\"/><source data-testid=\"og\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*r5TuRNbUKytQm--do3BgUA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*r5TuRNbUKytQm--do3BgUA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*r5TuRNbUKytQm--do3BgUA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*r5TuRNbUKytQm--do3BgUA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*r5TuRNbUKytQm--do3BgUA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*r5TuRNbUKytQm--do3BgUA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*r5TuRNbUKytQm--do3BgUA.jpeg 1400w\"/><img alt=\"Image of waterfall cascades\" class=\"bg ln lo c\" height=\"394\" loading=\"eager\" width=\"700\"/></picture></div></div><figcaption class=\"lp lq lr la lb ls lt be b bf z ey\">Cascade photo by Diana Dorčáková: <a class=\"af lu\" href=\"https://www.pexels.com/photo/landscape-photography-of-water-falls-1262803/\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://www.pexels.com/photo/landscape-photography-of-water-falls-1262803/</a></figcaption></figure><p class=\"pw-post-body-paragraph lv lw fg lx b ga ly lz ma gd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq ez bj\" id=\"0b16\">In <a class=\"af lu\" href=\"/@michaelgoitein/the-playing-to-win-framework-a-global-strategy-experts-proven-method-d3d12294e959\" rel=\"noopener\">Part I of this series</a>, we met <a class=\"af lu\" href=\"/@rogermartin\" rel=\"noopener\">Roger L. Martin</a>, became familiar with his long and successful career in strategy, and introduced the “Playing to Win” framework.</p><p class=\"pw-post-body-paragraph lv lw fg lx b ga ly lz ma gd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq ez bj\" id=\"b642\">In <a class=\"af lu\" href=\"/@michaelgoitein/the-playing-to-win-framework-part-ii-the-strategy-process-map-efaa53434ddc\" rel=\"noopener\">Part II</a>, we learned about and stepped through the Strategy Process Map, a 7-step approach combining the strengths of Design Thinking with the rigor of scientific hypothesis and testing.</p><p class=\"pw-post-body-paragraph lv lw fg lx b ga ly lz ma gd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq ez bj\" id=\"fde6\">Now, in Part III, we’ll dig into the crux of winning strategy formulation — stepping through the five choices that make up the Strategy Choice Cascade.</p><h1 class=\"mr ms fg be mt mu mv gc mw mx my gf mz na nb nc nd ne nf ng nh ni nj nk nl nm bj\" id=\"8eda\">How the Strategy Choice Cascade is Different</h1><p class=\"pw-post-body-paragraph lv lw fg lx b ga nn lz ma gd no mc md me np mg mh mi nq mk ml mm nr mo mp mq ez bj\" id=\"2f20\">How do strategy experts actually formulate strategy?</p><p class=\"pw-post-body-paragraph lv lw fg lx b ga ly lz ma gd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq ez bj\" id=\"dba0\">What kinds of questions do they ask when creating their strategies?</p><p class=\"pw-post-body-paragraph lv lw fg lx b ga ly lz ma gd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq ez bj\" id=\"5ffc\">Interestingly, the process tends to diverge into some flavor of either a “One Idea” strategy, or a massive data gathering and synthesis exercise resulting in hundreds of pages of “strategic plans” patterned on pre-made “playbooks.”</p><p class=\"pw-post-body-paragraph lv lw fg lx b ga ly lz ma gd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq ez bj\" id=\"7caf\">Let’s look at each.</p><h1 class=\"mr ms fg be mt mu mv gc mw mx my gf mz na nb nc nd ne nf ng nh ni nj nk nl nm bj\" id=\"5d81\">The “One Idea” Strategy</h1><p class=\"pw-post-body-paragraph lv lw fg lx b ga nn lz ma gd no mc md me np mg mh mi nq mk ml mm nr mo mp mq ez bj\" id=\"2381\">The “One Idea” approach is typically based on something that seems to be a “can’t miss,” genius idea.</p><p class=\"pw-post-body-paragraph lv lw fg lx b ga ly lz ma gd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq ez bj\" id=\"f58d\">While the history of business is littered with any number of failed startups, a recent notable example was the short-form streaming service “Quibi.” Touted as the “future of streaming content,” Quibi was the brainchild of former Disney exec and Dreamworks SKG co-founder Jeffrey Katzenberg and former HP CEO Meg Whitman.</p><p class=\"pw-post-body-paragraph lv lw fg lx b ga ly lz ma gd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq ez bj\" id=\"73ae\">Quibi promised to “revolutionize” short-form subscription video tailored to the mobile form factor for short-form video “on the go.”</p><figure class=\"ld le lf lg lh li la lb paragraph-image\"><div class=\"lj lk hh ll bg lm\" role=\"button\" tabindex=\"0\"><div class=\"la lb ns\"><picture><source sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 1400w\" type=\"image/webp\"/><source data-testid=\"og\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*i_H8_8NJRzLWB0yBBD1wpg.jpeg 1400w\"/><img alt=\"Quibi’s presentation at CES in 2019\" class=\"bg ln lo c\" height=\"394\" loading=\"lazy\" width=\"700\"/></picture></div></div><figcaption class=\"lp lq lr la lb ls lt be b bf z ey\">Quibi’s presentation at CES. Courtesy of <a class=\"af lu\" href=\"https://www.marketingdive.com/news/quibi-calls-it-quits-key-takeaways-from-a-lesson-in-how-not-to-build-a-str/587550/\" rel=\"noopener ugc nofollow\" target=\"_blank\">MarketingDrive</a></figcaption></figure><blockquote class=\"nt nu nv\"><p class=\"lv lw nw lx b ga ly lz ma gd mb mc md nx mf mg mh ny mj mk ml nz mn mo mp mq ez bj\" id=\"05da\"><strong class=\"lx fh\">“It’s unbelievable how many hours we all spend watching great TV content today and, separately, how much time we are consuming short-form content… so why can’t those two</strong>…</p></blockquote></div></div></div></div></section></div></div>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: <p class=\"be b bf z ey\">Member-only story</p>\n",
      "h1\n",
      "h2\n",
      "Skipping: <p class=\"be b hk hl bj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar hm\" data-testid=\"authorName\" href=\"/@michaelgoitein?source=post_page-----279c3b5ccea3--------------------------------\" rel=\"noopener follow\">Michael Goitein</a></p>\n",
      "Skipping: <p class=\"be b hk hl ey\"><span><a class=\"hp hq ah ai aj ak al am an ao ap aq ar hr hs ht\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68c04f19aae2&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40michaelgoitein%2Fthe-playing-to-win-framework-part-iii-the-strategy-choice-cascade-279c3b5ccea3&amp;user=Michael+Goitein&amp;userId=68c04f19aae2&amp;source=post_page-68c04f19aae2----279c3b5ccea3---------------------post_header-----------\" rel=\"noopener follow\">Follow</a></span></p>\n",
      "Skipping: <p class=\"be b kb z ey\"><span class=\"jl\">--</span></p>\n",
      "Skipping: <p class=\"be b bf z ey\">Share</p>\n",
      "h1\n",
      "Skipping: <p class=\"pw-post-body-paragraph lv lw fg lx b ga nn lz ma gd no mc md me np mg mh mi nq mk ml mm nr mo mp mq ez bj\" id=\"2f20\">How do strategy experts actually formulate strategy?</p>\n",
      "Skipping: <p class=\"pw-post-body-paragraph lv lw fg lx b ga ly lz ma gd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq ez bj\" id=\"7caf\">Let’s look at each.</p>\n",
      "h1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_pattern = re.compile(r\"h\\d\")\n",
    "text_elements = []\n",
    "for element in main_article.find_all([\"p\", \"h1\", \"h2\", \"h3\"]):\n",
    "    if (element.name == 'p' and len(element.text.split(' ')) > 8):\n",
    "        text_elements.append(element.text)\n",
    "    elif (header_pattern.match(element.name)  and len(element.text.split(' ')) > 3): \n",
    "        text_elements.append(element.text)\n",
    "    else: \n",
    "        print(f\"Skipping: {element}\")\n",
    "\n",
    "len(text_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael Goitein'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(attrs={\"data-testid\" : \"authorName\"}).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs = artilces[0].xpath(\"//p\")\n",
    "len(paragraphs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[64].text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
